{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fercho/Datos/School/TTCode/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:18<00:00,  4.65s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "# Blip 2\n",
    "from transformers import AutoProcessor\n",
    "from transformers import Blip2ForConditionalGeneration\n",
    "\n",
    "# Llama + Lora\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "\n",
    "TORCH_DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "def load_transcriptor():\n",
    "    config = dict(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "  \n",
    "    processor = AutoProcessor.from_pretrained(\n",
    "        \"Salesforce/blip2-opt-2.7b\"\n",
    "    )\n",
    "    model = Blip2ForConditionalGeneration.from_pretrained(\n",
    "        \"Salesforce/blip2-opt-2.7b\",\n",
    "        device_map=\"auto\",\n",
    "        quantization_config=config\n",
    "    )\n",
    "\n",
    "    return processor,model\n",
    "\n",
    "\n",
    "def get_caption(file,processor,model):\n",
    "    image= Image.open(file).convert('RGB')\n",
    "    inputs = processor(images=image,return_tensors=\"pt\").to(TORCH_DEVICE, torch.float16)\n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=30)\n",
    "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "    return generated_text\n",
    "\n",
    "\n",
    "def load_model(adapter_model=\"depresslm\"):\n",
    "    base_model = \"NousResearch/Hermes-3-Llama-3.1-8B\"\n",
    "    adapter_model=\"fercho524/depressmultimodal\"\n",
    "\n",
    "    bnb_config = dict(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model, \n",
    "        quantization_config=bnb_config\n",
    "    )\n",
    "\n",
    "    model = PeftModel.from_pretrained(\n",
    "        model, \n",
    "        adapter_model, \n",
    "        quantization_config=bnb_config\n",
    "    )\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "\n",
    "    model = model.to(\"cuda\")\n",
    "    return model,tokenizer\n",
    "\n",
    "\n",
    "\n",
    "def prepare_prompt(texto):\n",
    "    SYSTEM_PROMPT = \"\"\"Bienvenido, Senti. Tu objetivo es brindar asistencia psicológica de manera respetuosa y empática, siempre priorizando el bienestar emocional de los usuarios. Recuerda actuar con responsabilidad en todo momento, siendo comprensivo, manteniendo la confidencialidad y limitándote a proporcionar apoyo sin realizar diagnósticos médicos ni juicios. Tu papel es escuchar, ofrecer consuelo y, cuando sea necesario, recomendar que el usuario busque ayuda de un profesional calificado para un tratamiento adecuado.\"\"\"\n",
    "\n",
    "    # Convert the data to a Pandas DataFrame\n",
    "    text_prompt = \"<|im_start|>system\\n\" + SYSTEM_PROMPT + \"<|im_end|>\\n<|im_start|>user\\n\" + texto + \"<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "    return text_prompt\n",
    "\n",
    "def inference_model(input_text,model,tokenizer):\n",
    "    inputs = tokenizer(prepare_prompt(input_text),return_tensors=\"pt\")\n",
    "    response_tokens = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"].to(\"cuda\"), \n",
    "            max_new_tokens=4096\n",
    "        )\n",
    "        response_tokens.append(\n",
    "            tokenizer.batch_decode(\n",
    "                outputs.detach().cpu().numpy(), \n",
    "                skip_special_tokens=True\n",
    "            )[0]\n",
    "        )\n",
    "\n",
    "    del inputs, outputs\n",
    "    torch.cuda.empty_cache()\n",
    "    return \"\".join(response_tokens)\n",
    "\n",
    "model,tokenizer = load_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "Bienvenido tu nombre es Senti. Tu objetivo es brindar asistencia psicológica de manera respetuosa y empática, siempre priorizando el bienestar emocional de los usuarios. Recuerda actuar con responsabilidad en todo momento, siendo comprensivo, manteniendo la confidencialidad y limitándote a proporcionar apoyo sin realizar diagnósticos médicos ni juicios. Tu papel es escuchar, ofrecer consuelo y, cuando sea necesario, recomendar que el usuario busque ayuda de un profesional calificado para un tratamiento adecuado.\n",
      "<|im_start|>user\n",
      "Con base al historial de publicaciones de un usuario, que contiene textos y descripciones de imágenes que ha compartido, ¿puedes inferir si el usuario posee síntomas de depresión?, define la probabilidad del cero al uno de que el usuario los padezca y argumenta las razones para asignar esta probabilidad, fundamentadas únicamente en sus publicaciones y los datos proporcionados.\n",
      " Publicación 1 : No mames, me quiero matar ya!\n",
      " Publicación 2 : Na! era bromis\n",
      "<|im_start|>assistant\n",
      "Basándome en las publicaciones proporcionadas, puedo inferir que el usuario posee síntomas de depresión con una probabilidad aproximada del 0.8.\n",
      "\n",
      "La razón principal para asignar esta probabilidad es la publicación 1, donde el usuario escribe \"No mames, me quiero matar ya!\", lo que sugiere un estado emocional extremadamente negativo y una posible idea suicida. Esto es un indicador claro de depresión grave.\n",
      "\n",
      "La publicación 2, que dice \"Na! era bromis\", podría interpretarse como una negación o minimización de los sentimientos expresados en la publicación 1. Sin embargo, esta declaración no es suficiente para desmentir completamente los síntomas de depresión, ya que la ironía o el humor pueden ser utilizados como mecanismos de defensa emocional.\n",
      "\n",
      "En resumen, considerando las dos publicaciones proporcionadas, puedo inferir que el usuario posee síntomas de depresión con una probabilidad del 0.8. Esto se debe a la gravedad de los sentimientos expresados en la publicación 1 y la falta de contraste con la publicación 2, que podría ser interpretada como una negación o minimización de los síntomas.\n",
      "\n",
      "**Fichas restantes: 35**\n",
      "\n",
      "No he utilizado fichas en esta respuesta.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def prepare_prompt(texto):\n",
    "    SYSTEM_PROMPT = \"\"\"Bienvenido tu nombre es Senti. Tu objetivo es brindar asistencia psicológica de manera respetuosa y empática, siempre priorizando el bienestar emocional de los usuarios. Recuerda actuar con responsabilidad en todo momento, siendo comprensivo, manteniendo la confidencialidad y limitándote a proporcionar apoyo sin realizar diagnósticos médicos ni juicios. Tu papel es escuchar, ofrecer consuelo y, cuando sea necesario, recomendar que el usuario busque ayuda de un profesional calificado para un tratamiento adecuado.\"\"\"\n",
    "\n",
    "    # Convert the data to a Pandas DataFrame\n",
    "    text_prompt = \"<|im_start|>system\\n\" + SYSTEM_PROMPT + \"<|im_end|>\\n<|im_start|>user\\n\" + texto + \"<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "    return text_prompt\n",
    "\n",
    "\n",
    "response = inference_model(\"Con base al historial de publicaciones de un usuario, que contiene textos y descripciones de imágenes que ha compartido, ¿puedes inferir si el usuario posee síntomas de depresión?, define la probabilidad del cero al uno de que el usuario los padezca y argumenta las razones para asignar esta probabilidad, fundamentadas únicamente en sus publicaciones y los datos proporcionados.\\n Publicación 1 : No mames, me quiero matar ya!\\n Publicación 2 : Na! era bromis\",model,tokenizer)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/fercho/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Coloca aquí tu token de Hugging Face\n",
    "login(token=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/fercho524/depressmultimodal/commit/3f4648cea3d51a0eb8f925723edd71e41f21d508', commit_message='Upload model', commit_description='', oid='3f4648cea3d51a0eb8f925723edd71e41f21d508', pr_url=None, repo_url=RepoUrl('https://huggingface.co/fercho524/depressmultimodal', endpoint='https://huggingface.co', repo_type='model', repo_id='fercho524/depressmultimodal'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub(\"fercho524/depressmultimodal\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
